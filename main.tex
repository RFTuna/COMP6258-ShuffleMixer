\documentclass{article}

\usepackage{cleveref}


\usepackage{iclr2022/iclr2022_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{iclr2022/math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Formatting Instructions for ICLR 2022 \\ Conference Submissions}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

    \maketitle

    \begin{abstract}
    The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
    right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
    The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
    line spaces precede the abstract. The abstract must be limited to one
    paragraph.
    \end{abstract}

    \section{Introduction}

    ShuffleMixer~\citep{sun2022shufflemixer}.

    Brief outline of original paper and core ideas - no more repetition

    \subsection{Target Questions}

    \subsection{Methodology}

    \section{Implementation Details}

    \subsection{Implementation from Paper}

    The first task was to implement the model using only the description in the original paper; the original implementation was used as a sanity check as described in \cref{subsec:comparison-with-original-implementation}.

    The original paper has a useful block diagram, Figure 2, that breaks the model down into different levels of detail. For example, the `Shuffle Mixer Layer' is shown to include two `Channel Projection' blocks with Depthwise Convolution inbetween. In a seperate section of the figure, the `Channel Projection' block is broken down.

    This lends itself well to splitting the implementation into separate blocks. Each such block was written as a seperate PyTorch Module, to make it simple to ensure the architecture is as described in the paper.

    The first line of their equation (1) is missing a LHS but the correct expression ($Z_0^1, Z_0^2$) can be inferred from the other lines, the aforementioned Figure and the fact that the line includes a $Split$ operation.

    For this channel splitting operation, no ratio could be found in the paper. As in the original channel splitting work by \citet{ma2018shufflenet} the channels are split evenly, and they split evenly in Figure 2, an even split was choosen.

    After the split tensor passes through a `Point-wise MLP' block it is concatanated with the other tensor and the channels are shuffled, using the operation given by \citet{zhang2017shufflenet}. The PyTorch library version of this operation is used.

    Some details are given only in text.

    The feature extraction is a single 3x3 convolution layer.

    The upsampling is `only a convolutional layer of size 1x1 and a pixel shuffling layer'.


    Further details are provided in their 4.1 Datasets and implementation section. 64X64 images are used as the input (though because it is convolutional, input width could perhaps be variable between batches and for inference). The images are RGB, so 3 input channels. The fundemental building block is repeated five times. Two different models are described ShuffleMixer, and ShuffleMixer-Tiny. The quotation below described the differences between them:

    \begin{quotation}
        The number of channels and convolution kernel sizes is 64 and
7 × 7 pixels for the ShuffleMixer model and 32 and 3 × 3 pixels for the ShuffleMixer-Tiny model.
    \end{quotation}

    We fit these to the two missing values - the number of features and depth convolution size, but this is probably the part of the paper that was least clear. After all, at different points in the model the number of channels varies - and there are many different convolutions in the model.

    The implementation of the depth convultion, as group convolution with a number of groups equal to the number of input channels, was found in \citet{liu2022convnet}.


    A sanity check was done with TorchInfo, and convolultion padding issues were fixed. No other issues were found with the architecture.

    \subsection{Comparison with Original Implementation}\label{subsec:comparison-with-original-implementation}

    Unfortantely, their implementation does not perfectly follow the block structure they provide in Figure 2. This makes sense because a number of the conceptual blocks are very simple, and can be merged without producing complex code. It makes comparing slightly more difficult. An extreme example is the`feature extraction' block, which consists of nothing but a single conv layer.

    Confirmation:

    Their version allows control over the ratio, but it is set to 2.

    They implement a layer norm that includes bias and gain as parameters

    the norms and addition are split from the Mlp, but the order is the same.

    `kernel size' does indeed mean depth kernel size, same for features/channels.

    Differences:

    They also supprt x3 scaling, not mentioned in the paper.

    Their upsampling block finishes with SiLU. This isn't mentioned in the paper.

    We keep those differences.


%    https://github.com/sunny2109/ShuffleMixer/blob/39b79db88cac19219bba035a4dbd12af5afb31b8/basicsr/archs/shufflemixer_arch.py


    \section{Training}

    using the same data, DF2K, which is the HR images from DIV2k \citep{Agustsson_2017_CVPR_Workshops} and Flickr2K. Wierd issues finding a reference for this dataset - as the referenced paper seems unrelated (uses DIV2K). Shady download from git, but works.

    %https://github.com/limbee/NTIRE2017/issues/25

    Actually those links dont work

    Have to use a script and creat a Flickr API key - honestly ridculous

    Found download link:

    % https://cvnote.ddlee.cc/2019/09/22/image-super-resolution-datasets

    FFT parameters for loss not given, used torch defaults (and fft2).

    Adam paratemers also not specfied, use defaults again.

    Hit an issue! derivative for channel_shuffle is not implemented

    discussed here:
    %https://github.com/pytorch/pytorch/issues/67240

    Without using distributed training for simplicties sake,
    home computer is more useful than remote.

    However, memory cannot handle batch size of 64 when doing 4X scaling.
    So double the number of epochs, and se batch size to 32.
    and times the learning rate by sqrt(1/2) \citep{krizhevsky2014weird}.



    \section{Analysis}



    \bibliography{main}
    \bibliographystyle{iclr2022/iclr2022_conference}

\end{document}